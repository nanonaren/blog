    [BLOpts]
    profile    = nanonaren
    postid     = 1135
    title      = "Paper: Attention Is All You Need (63/365)"
    tags       = daily, language, nlp
    categories = [Papers]

I can't believe after my gripe yesterday that I have picked today's
paper. This paper claims that a feed-forward network with
self-attention trains faster and performs better than a recurrent or a
convolutional neural network on translation tasks. I have to come back
to this at a later time though because I need to brush up on much of
the terminology.
